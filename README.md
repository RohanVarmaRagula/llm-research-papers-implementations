# llm-research-papers-implementations
This repository contains my personal implementations and summaries of some of the most influential research papers that shaped the development of large language models (LLMs). It's designed as a learning and reference tool for researchers, engineers, and students.

## ğŸ“š Papers Covered

| #  | Paper Title                                                                                  | Authors       | Year | Code Status   | Summary Status |
|----|----------------------------------------------------------------------------------------------|---------------|------|---------------|----------------|
| 1  | Attention Is All You Need                                                                    | Vaswani et al. | 2017 | ğŸš§ In Progress| ğŸ“ To-do     |
| 2  | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding             | Devlin et al.  | 2018 | â³ To-do      | ğŸ“ To-do     |
| 3  | Improving Language Understanding by Generative Pre-Training (GPT)                            | Radford et al. | 2018 | â³ To-do      | ğŸ“ To-do     |
| 4  | Language Models are Unsupervised Multitask Learners (GPT-2)                                  | Radford et al. | 2019 | â³ To-do      | ğŸ“ To-do     |
| 5  | Fine-Tuning Language Models from Human Preferences                                           | Stiennon et al.| 2020 | â³ To-do      | ğŸ“ To-do     |
| 6  | Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | Fedus et al.   | 2021 | â³ To-do      | ğŸ“ To-do     |
| 7  | LoRA: Low-Rank Adaptation of Large Language Models                                           | Hu et al.      | 2021 | â³ To-do      | ğŸ“ To-do     |
| 8  | Instruction Tuning with GPT-3                                                                | Ouyang et al.  | 2022 | â³ To-do      | ğŸ“ To-do     |
| 9  | Chain-of-Thought Prompting Elicits Reasoning in LLMs                                         | Wei et al.     | 2022 | â³ To-do      | ğŸ“ To-do     |
| 10 | FLAN: Finetuned Language Models Are Zero-Shot Learners                                       | Chung et al.   | 2022 | â³ To-do      | ğŸ“ To-do     |
| 11 | LLaMA: Open and Efficient Foundation Language Models                                         | Touvron et al. | 2023 | â³ To-do      | ğŸ“ To-do     |
| 12 | Mixtral of Experts                                                                           | Mistral AI     | 2023 | â³ To-do      | ğŸ“ To-do     |


## ğŸ›  Tech Stack

- Python ğŸ
- PyTorch
- Jupyter for experiments

## ğŸ“¦ How to Use

```bash
git clone https://github.com/yourusername/llm-research-papers-implementations.git
cd llm-research-papers-implementations
# open and run notebooks or scripts as per paper folders
